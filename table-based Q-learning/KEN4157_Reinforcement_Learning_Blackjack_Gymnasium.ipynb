{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KEN4157 - Reinforcement Learning - Blackjack Gymnasium\n",
        "If you opened this notebook in Google Colab, we recommend to start by saving a copy of the notebook in your own Google Drive, such that you can save any of your changes and experiments."
      ],
      "metadata": {
        "id": "TZQ5cRaoNmYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing & Importing Modules\n",
        "We will start by installing importing some modules that will likely be useful for your assignment(s). This includes [Gymnasium](https://gymnasium.farama.org/), which is a framework containing many popular RL environments (a successor to the original Gym API from OpenAI)."
      ],
      "metadata": {
        "id": "tiobt1sTN2hG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "\n",
        "import gymnasium as gym\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Jd_DIXIdN5qT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "266ef58d-7655-4e17-f7dd-f955c817c670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numpy Cheat Sheet\n",
        "In case you are not already familiar with `numpy`, here are some examples of functions which may come in useful:"
      ],
      "metadata": {
        "id": "DWW904hT4rTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates a fixed-size array of 5 entries, each initialised to 0.0\n",
        "a = np.zeros(5)\n",
        "# Creates a 5 x 6 array (i.e., matrix) initialised to all-zeros\n",
        "b = np.zeros((5, 6))\n",
        "# Creates a 2 x 3 x 5 (i.e., 3-dimensional) array initialised to all-zeros\n",
        "c = np.zeros((2, 3, 5))\n",
        "\n",
        "# Samples a number uniformly at random from [0, 1)\n",
        "d = np.random.random()\n",
        "# Generates an array with 5 numbers, each sampled uniformly at random from [0, 1)\n",
        "e = np.random.random(5)\n",
        "\n",
        "# Gives you the maximum number from the array `e`\n",
        "f = np.max(e)\n",
        "# Gives you the index that holds the maximum number in the array `e`\n",
        "g = np.argmax(e)\n",
        "\n",
        "# Clear all the variables we created above, purely for the purpose of examples, from memory\n",
        "del a, b, c, d, e, f, g"
      ],
      "metadata": {
        "id": "wnFnb6Fn40A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the Blackjack Environment\n",
        "Here, we'll set up the Blackjack environment, and have a first look at how to interact with it according to the Gym API.\n",
        "\n",
        "**Optional**:\n",
        "- For a description and documentation of the environment, see: https://gymnasium.farama.org/environments/toy_text/blackjack/\n",
        "- For the implementation of the environment, see: https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/toy_text/blackjack.py"
      ],
      "metadata": {
        "id": "9J790RSkOXIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the environment. sab=True means we follow the rules exactly as described\n",
        "# in the Sutton and Barto book\n",
        "env = gym.make(\"Blackjack-v1\", sab=True)\n",
        "\n",
        "action_space = env.action_space\n",
        "obs_space = env.observation_space\n",
        "\n",
        "print(\"action space =\", action_space)\n",
        "# action space = Discrete(2): this means that we have two actions (0 and 1)\n",
        "\n",
        "# Let's create an array with nicer names for the actions (taken from documentation)\n",
        "ACTION_NAMES = [\"Stick\", \"Hit\"]\n",
        "\n",
        "print(\"observation space =\", obs_space)\n",
        "# observation space = Tuple(Discrete(32), Discrete(11), Discrete(2)):\n",
        "#  - The player's current sum can be any from a discrete set of 32 values\n",
        "#  - Value of the dealer's face-up card can be any from a discrete set of 11 values\n",
        "#  - Whether or not the player holds a usable ace is boolean (two possible values)\n",
        "\n",
        "print(f\"The first variable of observation space has {obs_space[0].n} possible values.\")\n",
        "print(f\"The second variable of observation space has {obs_space[1].n} possible values.\")\n",
        "print(f\"The third variable of observation space has {obs_space[2].n} possible values.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKhcxGIpnEkB",
        "outputId": "81212518-f46e-4c18-e6f2-35c493406ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action space = Discrete(2)\n",
            "observation space = Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
            "The first variable of observation space has 32 possible values.\n",
            "The second variable of observation space has 11 possible values.\n",
            "The third variable of observation space has 2 possible values.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Episodes with Random Policy\n",
        "We'll show how to run a few episodes under a random policy. This will demonstrate how to interact with the Gym API and its most important functions and return values."
      ],
      "metadata": {
        "id": "4Ya5I7CXsLnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(5):  # Run 5 episodes under random policy\n",
        "    obs, info = env.reset()     # Resets environment to initial state, gives us observation of initial state\n",
        "    done = False\n",
        "\n",
        "    returns = 0.0   # collect returns for this episode (note: not doing any discounting)\n",
        "\n",
        "    print(f\"The initial game state is: {obs}\")\n",
        "\n",
        "    # The following loop will run one complete episode\n",
        "    while not done:\n",
        "        print(f\"value of cards I'm holding = {obs[0]}\")\n",
        "        print(f\"value of dealer's face-up card = {obs[1]}\")\n",
        "        if obs[2]:\n",
        "            print(\"I have a usable ace\")\n",
        "        else:\n",
        "            print(\"I do not have a usable ace\")\n",
        "\n",
        "        action = action_space.sample()  # This randomly selects one action from the action space\n",
        "\n",
        "        print(\"I randomly chose to: \", ACTION_NAMES[action])\n",
        "\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action)    # Execute the action, observe successor state and reward\n",
        "\n",
        "        done = terminated or truncated\n",
        "        obs = next_obs\n",
        "        returns = returns + reward\n",
        "\n",
        "    print(f\"The episode ended with returns = {returns}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "QHQzSjrbsX6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table-Based Q-Learning\n",
        "Below, you can write your own table-based Q-learning implementation."
      ],
      "metadata": {
        "id": "uzyiZre5wKbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Q-learning"
      ],
      "metadata": {
        "id": "yALrc5eBwSUD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}