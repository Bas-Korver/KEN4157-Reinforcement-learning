{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# KEN4157 - Reinforcement Learning - Blackjack Gymnasium\n",
    "If you opened this notebook in Google Colab, we recommend to start by saving a copy of the notebook in your own Google Drive, such that you can save any of your changes and experiments."
   ],
   "metadata": {
    "id": "TZQ5cRaoNmYa"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installing & Importing Modules\n",
    "We will start by installing importing some modules that will likely be useful for your assignment(s). This includes [Gymnasium](https://gymnasium.farama.org/), which is a framework containing many popular RL environments (a successor to the original Gym API from OpenAI)."
   ],
   "metadata": {
    "id": "tiobt1sTN2hG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "id": "Jd_DIXIdN5qT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "266ef58d-7655-4e17-f7dd-f955c817c670",
    "ExecuteTime": {
     "end_time": "2024-05-27T10:45:49.132061Z",
     "start_time": "2024-05-27T10:45:48.839555Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Numpy Cheat Sheet\n",
    "In case you are not already familiar with `numpy`, here are some examples of functions which may come in useful:"
   ],
   "metadata": {
    "id": "DWW904hT4rTt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Creates a fixed-size array of 5 entries, each initialised to 0.0\n",
    "a = np.zeros(5)\n",
    "# Creates a 5 x 6 array (i.e., matrix) initialised to all-zeros\n",
    "b = np.zeros((5, 6))\n",
    "# Creates a 2 x 3 x 5 (i.e., 3-dimensional) array initialised to all-zeros\n",
    "c = np.zeros((2, 3, 5))\n",
    "\n",
    "# Samples a number uniformly at random from [0, 1)\n",
    "d = np.random.random()\n",
    "# Generates an array with 5 numbers, each sampled uniformly at random from [0, 1)\n",
    "e = np.random.random(5)\n",
    "\n",
    "# Gives you the maximum number from the array `e`\n",
    "f = np.max(e)\n",
    "# Gives you the index that holds the maximum number in the array `e`\n",
    "g = np.argmax(e)\n",
    "\n",
    "# Clear all the variables we created above, purely for the purpose of examples, from memory\n",
    "del a, b, c, d, e, f, g"
   ],
   "metadata": {
    "id": "wnFnb6Fn40A4",
    "ExecuteTime": {
     "end_time": "2024-05-27T10:45:49.140543Z",
     "start_time": "2024-05-27T10:45:49.133370Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up the Blackjack Environment\n",
    "Here, we'll set up the Blackjack environment, and have a first look at how to interact with it according to the Gym API.\n",
    "\n",
    "**Optional**:\n",
    "- For a description and documentation of the environment, see: https://gymnasium.farama.org/environments/toy_text/blackjack/\n",
    "- For the implementation of the environment, see: https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/toy_text/blackjack.py"
   ],
   "metadata": {
    "id": "9J790RSkOXIH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the environment. sab=True means we follow the rules exactly as described\n",
    "# in the Sutton and Barto book\n",
    "env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "\n",
    "action_space = env.action_space\n",
    "obs_space = env.observation_space\n",
    "\n",
    "print(\"action space =\", action_space)\n",
    "# action space = Discrete(2): this means that we have two actions (0 and 1)\n",
    "\n",
    "# Let's create an array with nicer names for the actions (taken from documentation)\n",
    "ACTION_NAMES = [\"Stick\", \"Hit\"]\n",
    "\n",
    "print(\"observation space =\", obs_space)\n",
    "# observation space = Tuple(Discrete(32), Discrete(11), Discrete(2)):\n",
    "#  - The player's current sum can be any from a discrete set of 32 values\n",
    "#  - Value of the dealer's face-up card can be any from a discrete set of 11 values\n",
    "#  - Whether or not the player holds a usable ace is boolean (two possible values)\n",
    "\n",
    "print(f\"The first variable of observation space has {obs_space[0].n} possible values.\")\n",
    "print(f\"The second variable of observation space has {obs_space[1].n} possible values.\")\n",
    "print(f\"The third variable of observation space has {obs_space[2].n} possible values.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XKhcxGIpnEkB",
    "outputId": "81212518-f46e-4c18-e6f2-35c493406ae4",
    "ExecuteTime": {
     "end_time": "2024-05-27T10:45:49.161261Z",
     "start_time": "2024-05-27T10:45:49.143576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space = Discrete(2)\n",
      "observation space = Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "The first variable of observation space has 32 possible values.\n",
      "The second variable of observation space has 11 possible values.\n",
      "The third variable of observation space has 2 possible values.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Episodes with Random Policy\n",
    "We'll show how to run a few episodes under a random policy. This will demonstrate how to interact with the Gym API and its most important functions and return values."
   ],
   "metadata": {
    "id": "4Ya5I7CXsLnr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for _ in range(1):  # Run 5 episodes under random policy\n",
    "    obs, info = env.reset()     # Resets environment to initial state, gives us observation of initial state\n",
    "    done = False\n",
    "\n",
    "    returns = 0.0   # collect returns for this episode (note: not doing any discounting)\n",
    "\n",
    "    print(f\"The initial game state is: {obs}\")\n",
    "\n",
    "    # The following loop will run one complete episode\n",
    "    while not done:\n",
    "        print(f\"value of cards I'm holding = {obs[0]}\")\n",
    "        print(f\"value of dealer's face-up card = {obs[1]}\")\n",
    "        if obs[2]:\n",
    "            print(\"I have a usable ace\")\n",
    "        else:\n",
    "            print(\"I do not have a usable ace\")\n",
    "\n",
    "        action = action_space.sample()  # This randomly selects one action from the action space\n",
    "\n",
    "        print(\"I randomly chose to: \", ACTION_NAMES[action])\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)    # Execute the action, observe successor state and reward\n",
    "        print(f\"{terminated=}, {truncated=}, {reward=}, {next_obs=}\")\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "        returns = returns + reward\n",
    "\n",
    "    print(f\"The episode ended with returns = {returns}\")\n",
    "    print()"
   ],
   "metadata": {
    "id": "QHQzSjrbsX6c",
    "ExecuteTime": {
     "end_time": "2024-05-27T10:45:49.172665Z",
     "start_time": "2024-05-27T10:45:49.163267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial game state is: (19, 3, 1)\n",
      "value of cards I'm holding = 19\n",
      "value of dealer's face-up card = 3\n",
      "I have a usable ace\n",
      "I randomly chose to:  Hit\n",
      "terminated=False, truncated=False, reward=0.0, next_obs=(21, 3, 1)\n",
      "value of cards I'm holding = 21\n",
      "value of dealer's face-up card = 3\n",
      "I have a usable ace\n",
      "I randomly chose to:  Stick\n",
      "terminated=True, truncated=False, reward=1.0, next_obs=(21, 3, 1)\n",
      "The episode ended with returns = 1.0\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Table-Based Q-Learning\n",
    "Below, you can write your own table-based Q-learning implementation."
   ],
   "metadata": {
    "id": "uzyiZre5wKbd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: Q-learning\n",
    "# Learn the best Q-values for any environment with Q-learning.\n",
    "def q_learning(q_values: np.ndarray, episode_num: int, game_env: gym.Env, alpha: float=0.1, gamma: float=1.0, epsilon: float=0.1) -> np.ndarray:\n",
    "    for _ in tqdm(range(episode_num)):\n",
    "        obs, info = game_env.reset()\n",
    "        while True:\n",
    "            if np.random.random() < epsilon:\n",
    "                action = action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(q_values[obs[0], obs[1], obs[2], :])\n",
    "                \n",
    "            next_obs, reward, terminated, truncated, info = game_env.step(action)\n",
    "            \n",
    "            if terminated:\n",
    "                value = 0\n",
    "            else:\n",
    "                value = np.max(q_values[next_obs[0], next_obs[1], next_obs[2], :])\n",
    "            \n",
    "            q_values[obs[0], obs[1], obs[2], action] += alpha * (reward + gamma * value - q_values[obs[0], obs[1], obs[2], action])\n",
    "            obs = next_obs\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "    return q_values\n",
    "        "
   ],
   "metadata": {
    "id": "yALrc5eBwSUD",
    "ExecuteTime": {
     "end_time": "2024-05-27T10:45:49.185470Z",
     "start_time": "2024-05-27T10:45:49.175672Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T10:45:49.194933Z",
     "start_time": "2024-05-27T10:45:49.188480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evalute the learned Q-values.\n",
    "def eval_game(q_values: np.ndarray, episode_num: int, game_env: gym.Env) -> float:\n",
    "    obs, info = game_env.reset()\n",
    "    reward = 0\n",
    "    for _ in tqdm(range(episode_num)):\n",
    "        while True:\n",
    "            action = np.argmax(q_values[obs[0], obs[1], obs[2], :])\n",
    "            next_obs, reward, terminated, truncated, info = game_env.step(action)\n",
    "            reward += reward\n",
    "            obs = next_obs\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "    return reward / episode_num"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T10:45:51.280224Z",
     "start_time": "2024-05-27T10:45:49.196941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "q_values = np.zeros((obs_space[0].n, obs_space[1].n, obs_space[2].n, action_space.n))\n",
    "episode_num = 10000\n",
    "env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "\n",
    "q_values = q_learning(q_values, episode_num, env, gamma=0.9, epsilon=1e-4)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 4836.43it/s]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T10:45:51.312930Z",
     "start_time": "2024-05-27T10:45:51.284232Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Average reward: {eval_game(q_values, 1000, env)}\")",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 58838.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: -0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T10:45:51.325473Z",
     "start_time": "2024-05-27T10:45:51.315937Z"
    }
   },
   "cell_type": "code",
   "source": "np.min(q_values), np.max(q_values)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.8702745079068737, 0.9942735831029776)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T10:45:51.332071Z",
     "start_time": "2024-05-27T10:45:51.327483Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 9
  }
 ]
}
